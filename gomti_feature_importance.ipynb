{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0e7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary py libraries\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf05e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is set as: P:\\IT\\Kaggle\\gomti_river\n",
      "Input files are under the folder: P:\\IT\\Kaggle\\gomti_river\\input\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gomti_Morphometry_23_Dec_2023.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check working directory/ set working directory/ set input folder path\n",
    "\n",
    "path = os.getcwd()\n",
    "print(\"Current working directory is set as:\", path)\n",
    "file_path = path+'\\\\input'\n",
    "print(\"Input files are under the folder:\", file_path)\n",
    "\n",
    "#List files under input folder\n",
    "os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f786b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input files\n",
    "\n",
    "df = pd.read_csv(file_path+'\\\\Gomti_Morphometry_23_Dec_2023.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8822e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SB</th>\n",
       "      <th>Lb</th>\n",
       "      <th>L avg.</th>\n",
       "      <th>Rb</th>\n",
       "      <th>Rl</th>\n",
       "      <th>A</th>\n",
       "      <th>Dd</th>\n",
       "      <th>Fs</th>\n",
       "      <th>Rt</th>\n",
       "      <th>Re</th>\n",
       "      <th>Ff</th>\n",
       "      <th>Rc</th>\n",
       "      <th>Lg</th>\n",
       "      <th>In</th>\n",
       "      <th>R</th>\n",
       "      <th>RR</th>\n",
       "      <th>Rn</th>\n",
       "      <th>HI</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SB1</td>\n",
       "      <td>9.59</td>\n",
       "      <td>0.373</td>\n",
       "      <td>3.717</td>\n",
       "      <td>2.046</td>\n",
       "      <td>32.32</td>\n",
       "      <td>3.19</td>\n",
       "      <td>8.54</td>\n",
       "      <td>8.23</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.157</td>\n",
       "      <td>27.21</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.115</td>\n",
       "      <td>3.524</td>\n",
       "      <td>0.438</td>\n",
       "      <td>44.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SB2</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.385</td>\n",
       "      <td>4.499</td>\n",
       "      <td>2.375</td>\n",
       "      <td>37.30</td>\n",
       "      <td>3.29</td>\n",
       "      <td>8.55</td>\n",
       "      <td>9.21</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.152</td>\n",
       "      <td>28.18</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>3.654</td>\n",
       "      <td>0.406</td>\n",
       "      <td>41.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SB3</td>\n",
       "      <td>15.63</td>\n",
       "      <td>0.427</td>\n",
       "      <td>4.154</td>\n",
       "      <td>2.023</td>\n",
       "      <td>44.50</td>\n",
       "      <td>3.40</td>\n",
       "      <td>7.98</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.147</td>\n",
       "      <td>27.15</td>\n",
       "      <td>1178</td>\n",
       "      <td>0.075</td>\n",
       "      <td>4.010</td>\n",
       "      <td>0.350</td>\n",
       "      <td>34.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SB4</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.488</td>\n",
       "      <td>4.333</td>\n",
       "      <td>2.347</td>\n",
       "      <td>14.47</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.57</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.156</td>\n",
       "      <td>21.01</td>\n",
       "      <td>671</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.148</td>\n",
       "      <td>0.271</td>\n",
       "      <td>29.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SB5</td>\n",
       "      <td>14.94</td>\n",
       "      <td>0.408</td>\n",
       "      <td>2.871</td>\n",
       "      <td>1.762</td>\n",
       "      <td>46.72</td>\n",
       "      <td>3.34</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.150</td>\n",
       "      <td>27.31</td>\n",
       "      <td>1475</td>\n",
       "      <td>0.099</td>\n",
       "      <td>4.926</td>\n",
       "      <td>0.273</td>\n",
       "      <td>34.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SB     Lb  L avg.     Rb     Rl      A    Dd    Fs    Rt     Re     Ff  \\\n",
       "0  SB1   9.59   0.373  3.717  2.046  32.32  3.19  8.54  8.23  0.669  0.351   \n",
       "1  SB2  10.96   0.385  4.499  2.375  37.30  3.29  8.55  9.21  0.629  0.310   \n",
       "2  SB3  15.63   0.427  4.154  2.023  44.50  3.40  7.98  8.12  0.482  0.182   \n",
       "3  SB4   9.69   0.488  4.333  2.347  14.47  3.20  6.57  3.83  0.443  0.154   \n",
       "4  SB5  14.94   0.408  2.871  1.762  46.72  3.34  8.18  6.26  0.516  0.209   \n",
       "\n",
       "      Rc     Lg     In     R     RR     Rn     HI      S  \n",
       "0  0.361  0.157  27.21  1106  0.115  3.524  0.438  44.51  \n",
       "1  0.390  0.152  28.18  1109  0.101  3.654  0.406  41.91  \n",
       "2  0.292  0.147  27.15  1178  0.075  4.010  0.350  34.26  \n",
       "3  0.296  0.156  21.01   671  0.069  2.148  0.271  29.46  \n",
       "4  0.157  0.150  27.31  1475  0.099  4.926  0.273  34.77  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1902f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13 entries, 0 to 12\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   SB      13 non-null     object \n",
      " 1   Lb      13 non-null     float64\n",
      " 2   L avg.  13 non-null     float64\n",
      " 3   Rb      13 non-null     float64\n",
      " 4   Rl      13 non-null     float64\n",
      " 5   A       13 non-null     float64\n",
      " 6   Dd      13 non-null     float64\n",
      " 7   Fs      13 non-null     float64\n",
      " 8   Rt      13 non-null     float64\n",
      " 9   Re      13 non-null     float64\n",
      " 10  Ff      13 non-null     float64\n",
      " 11  Rc      13 non-null     float64\n",
      " 12  Lg      13 non-null     float64\n",
      " 13  In      13 non-null     float64\n",
      " 14  R       13 non-null     int64  \n",
      " 15  RR      13 non-null     float64\n",
      " 16  Rn      13 non-null     float64\n",
      " 17  HI      13 non-null     float64\n",
      " 18  S       13 non-null     float64\n",
      "dtypes: float64(17), int64(1), object(1)\n",
      "memory usage: 2.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc1bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sweetviz as sv\n",
    "\n",
    "# my_report = sv.analyze(df)\n",
    "# my_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6be56c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lb</th>\n",
       "      <th>L avg.</th>\n",
       "      <th>Rb</th>\n",
       "      <th>Rl</th>\n",
       "      <th>A</th>\n",
       "      <th>Dd</th>\n",
       "      <th>Fs</th>\n",
       "      <th>Rt</th>\n",
       "      <th>Re</th>\n",
       "      <th>Ff</th>\n",
       "      <th>Rc</th>\n",
       "      <th>Lg</th>\n",
       "      <th>In</th>\n",
       "      <th>R</th>\n",
       "      <th>RR</th>\n",
       "      <th>Rn</th>\n",
       "      <th>HI</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.59</td>\n",
       "      <td>0.373</td>\n",
       "      <td>3.717</td>\n",
       "      <td>2.046</td>\n",
       "      <td>32.32</td>\n",
       "      <td>3.19</td>\n",
       "      <td>8.54</td>\n",
       "      <td>8.23</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.157</td>\n",
       "      <td>27.21</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.115</td>\n",
       "      <td>3.524</td>\n",
       "      <td>0.438</td>\n",
       "      <td>44.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.96</td>\n",
       "      <td>0.385</td>\n",
       "      <td>4.499</td>\n",
       "      <td>2.375</td>\n",
       "      <td>37.30</td>\n",
       "      <td>3.29</td>\n",
       "      <td>8.55</td>\n",
       "      <td>9.21</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.152</td>\n",
       "      <td>28.18</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>3.654</td>\n",
       "      <td>0.406</td>\n",
       "      <td>41.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.63</td>\n",
       "      <td>0.427</td>\n",
       "      <td>4.154</td>\n",
       "      <td>2.023</td>\n",
       "      <td>44.50</td>\n",
       "      <td>3.40</td>\n",
       "      <td>7.98</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.147</td>\n",
       "      <td>27.15</td>\n",
       "      <td>1178</td>\n",
       "      <td>0.075</td>\n",
       "      <td>4.010</td>\n",
       "      <td>0.350</td>\n",
       "      <td>34.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.69</td>\n",
       "      <td>0.488</td>\n",
       "      <td>4.333</td>\n",
       "      <td>2.347</td>\n",
       "      <td>14.47</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.57</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.156</td>\n",
       "      <td>21.01</td>\n",
       "      <td>671</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.148</td>\n",
       "      <td>0.271</td>\n",
       "      <td>29.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.94</td>\n",
       "      <td>0.408</td>\n",
       "      <td>2.871</td>\n",
       "      <td>1.762</td>\n",
       "      <td>46.72</td>\n",
       "      <td>3.34</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.150</td>\n",
       "      <td>27.31</td>\n",
       "      <td>1475</td>\n",
       "      <td>0.099</td>\n",
       "      <td>4.926</td>\n",
       "      <td>0.273</td>\n",
       "      <td>34.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lb  L avg.     Rb     Rl      A    Dd    Fs    Rt     Re     Ff     Rc  \\\n",
       "0   9.59   0.373  3.717  2.046  32.32  3.19  8.54  8.23  0.669  0.351  0.361   \n",
       "1  10.96   0.385  4.499  2.375  37.30  3.29  8.55  9.21  0.629  0.310  0.390   \n",
       "2  15.63   0.427  4.154  2.023  44.50  3.40  7.98  8.12  0.482  0.182  0.292   \n",
       "3   9.69   0.488  4.333  2.347  14.47  3.20  6.57  3.83  0.443  0.154  0.296   \n",
       "4  14.94   0.408  2.871  1.762  46.72  3.34  8.18  6.26  0.516  0.209  0.157   \n",
       "\n",
       "      Lg     In     R     RR     Rn     HI      S  \n",
       "0  0.157  27.21  1106  0.115  3.524  0.438  44.51  \n",
       "1  0.152  28.18  1109  0.101  3.654  0.406  41.91  \n",
       "2  0.147  27.15  1178  0.075  4.010  0.350  34.26  \n",
       "3  0.156  21.01   671  0.069  2.148  0.271  29.46  \n",
       "4  0.150  27.31  1475  0.099  4.926  0.273  34.77  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.drop(['SB'], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5437dc2",
   "metadata": {},
   "source": [
    "### Weights by using CRITIC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be60e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate weights\n",
    "\n",
    "def calculate_criteria_importance(matrix):\n",
    "  \"\"\"Calculates the importance of criteria using the Criteria Importance Through Intercriteria Correlation (CRITIC) method.\n",
    "\n",
    "  Args:\n",
    "    matrix: A decision matrix with criteria in the rows and alternatives in the columns.\n",
    "\n",
    "  Returns:\n",
    "    A vector of criteria importance weights.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate the criteria correlation matrix.\n",
    "  correlation_matrix = np.corrcoef(matrix)\n",
    "\n",
    "  # Calculate the criteria standard deviations.\n",
    "  standard_deviations = np.std(matrix, axis=1)\n",
    "\n",
    "  # Calculate the criteria importance weights.\n",
    "  weights = np.sum(correlation_matrix * standard_deviations, axis=1)\n",
    "\n",
    "  # Normalize the weights so that they sum to 1.\n",
    "  weights /= np.sum(weights)\n",
    "\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8f6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using crispyn library\n",
    "#https://pypi.org/project/crispyn/\n",
    "#https://www.sciencedirect.com/search?qs=critic_weighting\n",
    "\n",
    "import numpy as np\n",
    "from crispyn.mcda_methods import VIKOR\n",
    "from crispyn import weighting_methods as mcda_weights\n",
    "from crispyn import normalizations as norms\n",
    "from crispyn.additions import rank_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2aecf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SB1', 'SB2', 'SB3', 'SB4', 'SB5', 'SB6', 'SB7', 'SB8', 'SB9',\n",
       "       'SB10', 'SB11', 'SB12', 'SB13'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alter = df['SB'].to_numpy()\n",
    "alter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cee25da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lb', 'L avg.', 'Rb', 'Rl', 'A', 'Dd', 'Fs', 'Rt', 'Re', 'Ff',\n",
       "       'Rc', 'Lg', 'In', 'R', 'RR', 'Rn', 'HI', 'S'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = (train.columns.to_numpy())\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b13fe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lb' 'L avg.' 'Rb' 'Rl' 'A' 'Dd' 'Fs' 'Rt' 'Re' 'Ff' 'Rc' 'Lg' 'In' 'R'\n",
      " 'RR' 'Rn' 'HI' 'S']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d506a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.590e+00 3.730e-01 3.717e+00 2.046e+00 3.232e+01 3.190e+00 8.540e+00\n",
      "  8.230e+00 6.690e-01 3.510e-01 3.610e-01 1.570e-01 2.721e+01 1.106e+03\n",
      "  1.150e-01 3.524e+00 4.380e-01 4.451e+01]\n",
      " [1.096e+01 3.850e-01 4.499e+00 2.375e+00 3.730e+01 3.290e+00 8.550e+00\n",
      "  9.210e+00 6.290e-01 3.100e-01 3.900e-01 1.520e-01 2.818e+01 1.109e+03\n",
      "  1.010e-01 3.654e+00 4.060e-01 4.191e+01]\n",
      " [1.563e+01 4.270e-01 4.154e+00 2.023e+00 4.450e+01 3.400e+00 7.980e+00\n",
      "  8.120e+00 4.820e-01 1.820e-01 2.920e-01 1.470e-01 2.715e+01 1.178e+03\n",
      "  7.500e-02 4.010e+00 3.500e-01 3.426e+01]\n",
      " [9.690e+00 4.880e-01 4.333e+00 2.347e+00 1.447e+01 3.200e+00 6.570e+00\n",
      "  3.830e+00 4.430e-01 1.540e-01 2.960e-01 1.560e-01 2.101e+01 6.710e+02\n",
      "  6.900e-02 2.148e+00 2.710e-01 2.946e+01]\n",
      " [1.494e+01 4.080e-01 2.871e+00 1.762e+00 4.672e+01 3.340e+00 8.180e+00\n",
      "  6.260e+00 5.160e-01 2.090e-01 1.570e-01 1.500e-01 2.731e+01 1.475e+03\n",
      "  9.900e-02 4.926e+00 2.730e-01 3.477e+01]\n",
      " [1.046e+01 3.760e-01 3.908e+00 2.083e+00 3.042e+01 3.140e+00 8.350e+00\n",
      "  7.250e+00 5.950e-01 2.780e-01 3.110e-01 1.590e-01 2.621e+01 1.250e+03\n",
      "  1.190e-01 3.924e+00 4.380e-01 5.024e+01]\n",
      " [9.800e+00 3.960e-01 3.648e+00 1.905e+00 2.610e+01 3.390e+00 8.550e+00\n",
      "  7.340e+00 5.880e-01 2.720e-01 3.560e-01 1.480e-01 2.895e+01 9.030e+02\n",
      "  9.200e-02 3.059e+00 4.810e-01 3.952e+01]\n",
      " [9.150e+00 4.140e-01 2.324e+00 1.572e+00 2.415e+01 3.410e+00 8.240e+00\n",
      "  5.920e+00 6.060e-01 2.890e-01 2.690e-01 1.460e-01 2.814e+01 1.045e+03\n",
      "  1.140e-01 3.568e+00 2.810e-01 3.245e+01]\n",
      " [1.017e+01 3.810e-01 2.304e+00 1.789e+00 2.877e+01 2.920e+00 7.650e+00\n",
      "  6.700e+00 5.950e-01 2.780e-01 3.350e-01 1.710e-01 2.230e+01 1.195e+03\n",
      "  1.180e-01 3.484e+00 4.040e-01 5.107e+01]\n",
      " [8.570e+00 4.500e-01 3.499e+00 1.764e+00 2.318e+01 3.550e+00 7.890e+00\n",
      "  7.020e+00 6.340e-01 3.150e-01 4.290e-01 1.410e-01 2.802e+01 8.390e+02\n",
      "  9.800e-02 2.979e+00 4.330e-01 3.215e+01]\n",
      " [7.670e+00 4.060e-01 4.310e+00 2.192e+00 1.305e+01 3.390e+00 8.350e+00\n",
      "  5.330e+00 5.310e-01 2.220e-01 3.920e-01 1.470e-01 2.833e+01 1.209e+03\n",
      "  1.580e-01 4.102e+00 3.920e-01 4.094e+01]\n",
      " [1.437e+01 3.820e-01 2.396e+00 1.763e+00 3.093e+01 3.280e+00 8.570e+00\n",
      "  6.660e+00 4.370e-01 1.500e-01 2.450e-01 1.530e-01 2.807e+01 1.030e+03\n",
      "  7.200e-02 3.374e+00 3.850e-01 4.335e+01]\n",
      " [1.021e+01 3.870e-01 3.485e+00 2.033e+00 2.129e+01 3.160e+00 8.170e+00\n",
      "  5.730e+00 5.100e-01 2.040e-01 2.900e-01 1.580e-01 2.584e+01 1.218e+03\n",
      "  1.190e-01 3.851e+00 4.790e-01 4.562e+01]]\n"
     ]
    }
   ],
   "source": [
    "mat = train.to_numpy()\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c778593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [W_method, Lb, L avg., Rb, Rl, A, Dd, Fs, Rt, Re, Ff, Rc, Lg, In, R, RR, Rn, HI, S]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Initialize a DF to store weights calculated using various techniques\n",
    "\n",
    "wdf = pd.DataFrame(columns=features)\n",
    "wdf.insert(0, \"W_method\",[])\n",
    "print(wdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2f04a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wt_fun', 0.07692630031110961, 0.0769250648560987, 0.07691893903519288, 0.07692166268636254, 0.07691822059818403, 0.0769260013557528, 0.0769234214249206, 0.07692534621885935, 0.07692403930987936, 0.076924175137105, 0.07691786592899613, 0.07692493736120896, 0.07692402577633009]\n"
     ]
    }
   ],
   "source": [
    "# #Based on function\n",
    "# wt_fun = calculate_criteria_importance(mat)\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "# my_list = [\"wt_fun\"]\n",
    "# my_list.extend(wt_fun)\n",
    "\n",
    "# wdf.loc[len(wdf)] = my_list\n",
    "# wdf.head()\n",
    "\n",
    "#print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5611fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Entropy\n",
    "\n",
    "entropy = mcda_weights.entropy_weighting(mat)\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"entropy\"]\n",
    "my_list.extend(entropy)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list\n",
    "#wdf.head()\n",
    "#print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a148e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Equal\n",
    "\n",
    "equal = mcda_weights.equal_weighting(mat)\n",
    "#equal\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"equal\"]\n",
    "my_list.extend(equal)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list\n",
    "#wdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6ffe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Standard Devialtion\n",
    "\n",
    "std = mcda_weights.std_weighting(mat)\n",
    "#std\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"Standard_Dev\"]\n",
    "my_list.extend(std)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1614a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#CRITIC\n",
    "\n",
    "crit = mcda_weights.critic_weighting(mat)\n",
    "crit\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"CRITIC\"]\n",
    "my_list.extend(crit)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "691f5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Gini coefficient-based\n",
    "\n",
    "gin = mcda_weights.gini_weighting(mat)\n",
    "gin\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"Gini_coef\"]\n",
    "my_list.extend(gin)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3880e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#MEREC\n",
    "#Merec method requires type (Influence positive or negative)\n",
    "#Assumption that: Increasing the value Lg typically shall result in greater runoff and soil erosion from the respective sub-watershed, etc. Hence, sub-watershed with lower Lg value should be given higher priority and vice-versa.\n",
    " \n",
    "types = np.array([1, 1, 1, 1, 1, 1, 1,1,1,1,1,-1,1,1,1,1,1,1])\n",
    "\n",
    "mer = mcda_weights.merec_weighting(mat, types)\n",
    "mer\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"MEREC\"]\n",
    "my_list.extend(mer)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea1a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Statistical variance\n",
    "\n",
    "svar = mcda_weights.stat_var_weighting(mat)\n",
    "svar\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"Statistical_variance\"]\n",
    "my_list.extend(svar)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b70cd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#CILOS\n",
    "# Requires \"Types\" parameter\n",
    "\n",
    "cil = mcda_weights.cilos_weighting(mat, types)\n",
    "cil\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"CILOS\"]\n",
    "my_list.extend(cil)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb611a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#IDOCRIW\n",
    "# Requires \"Types\" parameter\n",
    "\n",
    "ido = mcda_weights.idocriw_weighting(mat, types)\n",
    "ido\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"IDOCRIW\"]\n",
    "my_list.extend(ido)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b06cffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Angle\n",
    "# Requires \"Types\" parameter\n",
    "\n",
    "ang = mcda_weights.angle_weighting(mat, types)\n",
    "ang\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"Angle\"]\n",
    "my_list.extend(ang)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4456583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different weights based on crispyn\n",
    "\n",
    "#Coefficient of variation\n",
    "\n",
    "cov = mcda_weights.coeff_var_weighting(mat)\n",
    "cov\n",
    "\n",
    "# #---------------------- Save the method and results to wdf --------------------------------------\n",
    "my_list = [\"CoV\"]\n",
    "my_list.extend(cov)\n",
    "\n",
    "wdf.loc[len(wdf)] = my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "078ed6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_method</th>\n",
       "      <th>Lb</th>\n",
       "      <th>L avg.</th>\n",
       "      <th>Rb</th>\n",
       "      <th>Rl</th>\n",
       "      <th>A</th>\n",
       "      <th>Dd</th>\n",
       "      <th>Fs</th>\n",
       "      <th>Rt</th>\n",
       "      <th>Re</th>\n",
       "      <th>Ff</th>\n",
       "      <th>Rc</th>\n",
       "      <th>Lg</th>\n",
       "      <th>In</th>\n",
       "      <th>R</th>\n",
       "      <th>RR</th>\n",
       "      <th>Rn</th>\n",
       "      <th>HI</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entropy</td>\n",
       "      <td>0.077536</td>\n",
       "      <td>0.009993</td>\n",
       "      <td>0.081475</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.194275</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.068204</td>\n",
       "      <td>0.028327</td>\n",
       "      <td>0.107551</td>\n",
       "      <td>0.084653</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.013033</td>\n",
       "      <td>0.055847</td>\n",
       "      <td>0.083395</td>\n",
       "      <td>0.053526</td>\n",
       "      <td>0.057344</td>\n",
       "      <td>0.046955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>equal</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Standard_Dev</td>\n",
       "      <td>0.010848</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.043856</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.006038</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.886709</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.030258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRITIC</td>\n",
       "      <td>0.069455</td>\n",
       "      <td>0.079098</td>\n",
       "      <td>0.069845</td>\n",
       "      <td>0.061986</td>\n",
       "      <td>0.055766</td>\n",
       "      <td>0.054823</td>\n",
       "      <td>0.043582</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.054860</td>\n",
       "      <td>0.054803</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.057618</td>\n",
       "      <td>0.052360</td>\n",
       "      <td>0.045219</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.041775</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.058529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gini_coef</td>\n",
       "      <td>0.071766</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.074887</td>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.117617</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>0.087950</td>\n",
       "      <td>0.074534</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>0.026222</td>\n",
       "      <td>0.060484</td>\n",
       "      <td>0.075581</td>\n",
       "      <td>0.058371</td>\n",
       "      <td>0.062235</td>\n",
       "      <td>0.059067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MEREC</td>\n",
       "      <td>0.052087</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>0.062371</td>\n",
       "      <td>0.035185</td>\n",
       "      <td>0.115441</td>\n",
       "      <td>0.018392</td>\n",
       "      <td>0.032939</td>\n",
       "      <td>0.085455</td>\n",
       "      <td>0.036377</td>\n",
       "      <td>0.073132</td>\n",
       "      <td>0.108195</td>\n",
       "      <td>0.018202</td>\n",
       "      <td>0.036875</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>0.060306</td>\n",
       "      <td>0.078314</td>\n",
       "      <td>0.052987</td>\n",
       "      <td>0.045817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Statistical_variance</td>\n",
       "      <td>0.063620</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>0.082723</td>\n",
       "      <td>0.058541</td>\n",
       "      <td>0.058112</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.047829</td>\n",
       "      <td>0.043150</td>\n",
       "      <td>0.067182</td>\n",
       "      <td>0.066201</td>\n",
       "      <td>0.045042</td>\n",
       "      <td>0.042193</td>\n",
       "      <td>0.058760</td>\n",
       "      <td>0.041662</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>0.036131</td>\n",
       "      <td>0.078184</td>\n",
       "      <td>0.067153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CILOS</td>\n",
       "      <td>0.024881</td>\n",
       "      <td>0.086620</td>\n",
       "      <td>0.026061</td>\n",
       "      <td>0.031597</td>\n",
       "      <td>0.024764</td>\n",
       "      <td>0.110816</td>\n",
       "      <td>0.194489</td>\n",
       "      <td>0.023583</td>\n",
       "      <td>0.044545</td>\n",
       "      <td>0.025127</td>\n",
       "      <td>0.029833</td>\n",
       "      <td>0.111052</td>\n",
       "      <td>0.110556</td>\n",
       "      <td>0.030332</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>0.029825</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>0.043207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IDOCRIW</td>\n",
       "      <td>0.062098</td>\n",
       "      <td>0.027863</td>\n",
       "      <td>0.068349</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.154861</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.051776</td>\n",
       "      <td>0.040617</td>\n",
       "      <td>0.086990</td>\n",
       "      <td>0.081294</td>\n",
       "      <td>0.013735</td>\n",
       "      <td>0.046383</td>\n",
       "      <td>0.054527</td>\n",
       "      <td>0.049481</td>\n",
       "      <td>0.051388</td>\n",
       "      <td>0.063278</td>\n",
       "      <td>0.065305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Angle</td>\n",
       "      <td>0.075075</td>\n",
       "      <td>0.027065</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.040448</td>\n",
       "      <td>0.112512</td>\n",
       "      <td>0.016201</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.067570</td>\n",
       "      <td>0.044348</td>\n",
       "      <td>0.084593</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.016116</td>\n",
       "      <td>0.029711</td>\n",
       "      <td>0.061232</td>\n",
       "      <td>0.076229</td>\n",
       "      <td>0.060174</td>\n",
       "      <td>0.061946</td>\n",
       "      <td>0.057222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CoV</td>\n",
       "      <td>0.075144</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>0.073415</td>\n",
       "      <td>0.040026</td>\n",
       "      <td>0.114944</td>\n",
       "      <td>0.015971</td>\n",
       "      <td>0.021893</td>\n",
       "      <td>0.067425</td>\n",
       "      <td>0.043927</td>\n",
       "      <td>0.085043</td>\n",
       "      <td>0.073998</td>\n",
       "      <td>0.016407</td>\n",
       "      <td>0.029338</td>\n",
       "      <td>0.060959</td>\n",
       "      <td>0.076338</td>\n",
       "      <td>0.059884</td>\n",
       "      <td>0.061685</td>\n",
       "      <td>0.056889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                W_method        Lb    L avg.        Rb        Rl         A  \\\n",
       "0                entropy  0.077536  0.009993  0.081475  0.023086  0.194275   \n",
       "1                  equal  0.055556  0.055556  0.055556  0.055556  0.055556   \n",
       "2           Standard_Dev  0.010848  0.000144  0.003411  0.001050  0.043856   \n",
       "3                 CRITIC  0.069455  0.079098  0.069845  0.061986  0.055766   \n",
       "4              Gini_coef  0.071766  0.025342  0.074887  0.040997  0.117617   \n",
       "5                  MEREC  0.052087  0.013426  0.062371  0.035185  0.115441   \n",
       "6   Statistical_variance  0.063620  0.053715  0.082723  0.058541  0.058112   \n",
       "7                  CILOS  0.024881  0.086620  0.026061  0.031597  0.024764   \n",
       "8                IDOCRIW  0.062098  0.027863  0.068349  0.023480  0.154861   \n",
       "9                  Angle  0.075075  0.027065  0.073400  0.040448  0.112512   \n",
       "10                   CoV  0.075144  0.026714  0.073415  0.040026  0.114944   \n",
       "\n",
       "          Dd        Fs        Rt        Re        Ff        Rc        Lg  \\\n",
       "0   0.003722  0.007236  0.068204  0.028327  0.107551  0.084653  0.003842   \n",
       "1   0.055556  0.055556  0.055556  0.055556  0.055556  0.055556  0.055556   \n",
       "2   0.000697  0.002363  0.006038  0.000325  0.000279  0.000312  0.000033   \n",
       "3   0.054823  0.043582  0.040001  0.054860  0.054803  0.049745  0.057618   \n",
       "4   0.015896  0.019231  0.068337  0.045288  0.087950  0.074534  0.016195   \n",
       "5   0.018392  0.032939  0.085455  0.036377  0.073132  0.108195  0.018202   \n",
       "6   0.041870  0.047829  0.043150  0.067182  0.066201  0.045042  0.042193   \n",
       "7   0.110816  0.194489  0.023583  0.044545  0.025127  0.029833  0.111052   \n",
       "8   0.013275  0.045300  0.051776  0.040617  0.086990  0.081294  0.013735   \n",
       "9   0.016201  0.022195  0.067570  0.044348  0.084593  0.073964  0.016116   \n",
       "10  0.015971  0.021893  0.067425  0.043927  0.085043  0.073998  0.016407   \n",
       "\n",
       "          In         R        RR        Rn        HI         S  \n",
       "0   0.013033  0.055847  0.083395  0.053526  0.057344  0.046955  \n",
       "1   0.055556  0.055556  0.055556  0.055556  0.055556  0.055556  \n",
       "2   0.010400  0.886709  0.000105  0.002853  0.000317  0.030258  \n",
       "3   0.052360  0.045219  0.050024  0.041775  0.060511  0.058529  \n",
       "4   0.026222  0.060484  0.075581  0.058371  0.062235  0.059067  \n",
       "5   0.036875  0.074498  0.060306  0.078314  0.052987  0.045817  \n",
       "6   0.058760  0.041662  0.047932  0.036131  0.078184  0.067153  \n",
       "7   0.110556  0.030332  0.018432  0.029825  0.034280  0.043207  \n",
       "8   0.046383  0.054527  0.049481  0.051388  0.063278  0.065305  \n",
       "9   0.029711  0.061232  0.076229  0.060174  0.061946  0.057222  \n",
       "10  0.029338  0.060959  0.076338  0.059884  0.061685  0.056889  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9038d9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fc8ff06f02450dab03656a273a0279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |                                             | [  0%]   00:00 ->â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report SWEETVIZ_REPORT.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "wt_report = sv.analyze(wdf)\n",
    "wt_report.show_html() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4427698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the WDF to excel\n",
    "file_name = 'Param_weights_multi_methods_02Jan24.xlsx'\n",
    " \n",
    "# saving the excel\n",
    "wdf.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb390594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Influence positive or negative\n",
    "types = np.array([1, 1, 1, 1, 1, 1, 1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "# Create the VIKOR method object\n",
    "vikor = VIKOR(normalization_method=norms.minmax_normalization)\n",
    "# Calculate alternatives preference function values with VIKOR method\n",
    "pref = vikor(mat, weights, types)\n",
    "# Rank alternatives according to preference values\n",
    "rank = rank_preferences(pref, reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68b4b41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  1,  4, 13,  5,  3,  7, 11,  6,  8, 12,  9, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f381ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfd30cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrices for model evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import  precision_score, recall_score, f1_score, precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, auc, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e134df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearSVR in module sklearn.svm._classes:\n",
      "\n",
      "class LinearSVR(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      " |  LinearSVR(*, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual='warn', verbose=0, random_state=None, max_iter=1000)\n",
      " |  \n",
      " |  Linear Support Vector Regression.\n",
      " |  \n",
      " |  Similar to SVR with parameter kernel='linear', but implemented in terms of\n",
      " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      " |  penalties and loss functions and should scale better to large numbers of\n",
      " |  samples.\n",
      " |  \n",
      " |  This class supports both dense and sparse input.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_regression>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.16\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  epsilon : float, default=0.0\n",
      " |      Epsilon parameter in the epsilon-insensitive loss function. Note\n",
      " |      that the value of this parameter depends on the scale of the target\n",
      " |      variable y. If unsure, set ``epsilon=0``.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Regularization parameter. The strength of the regularization is\n",
      " |      inversely proportional to C. Must be strictly positive.\n",
      " |  \n",
      " |  loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'\n",
      " |      Specifies the loss function. The epsilon-insensitive loss\n",
      " |      (standard SVR) is the L1 loss, while the squared epsilon-insensitive\n",
      " |      loss ('squared_epsilon_insensitive') is the L2 loss.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be already centered).\n",
      " |  \n",
      " |  intercept_scaling : float, default=1.0\n",
      " |      When self.fit_intercept is True, instance vector x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equals to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  dual : \"auto\" or bool, default=True\n",
      " |      Select the algorithm to either solve the dual or primal\n",
      " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      " |      `dual=\"auto\"` will choose the value of the parameter automatically,\n",
      " |      based on the values of `n_samples`, `n_features` and `loss`. If\n",
      " |      `n_samples` < `n_features` and optmizer supports chosen `loss`,\n",
      " |      then dual will be set to True, otherwise it will be set to False.\n",
      " |  \n",
      " |      .. versionchanged:: 1.3\n",
      " |         The default value will change from `True` to `\"auto\"` in 1.5.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the pseudo random number generation for shuffling the data.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of iterations to be run.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem).\n",
      " |  \n",
      " |      `coef_` is a readonly property derived from `raw_coef_` that\n",
      " |      follows the internal memory layout of liblinear.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Maximum number of iterations run across all classes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  LinearSVC : Implementation of Support Vector Machine classifier using the\n",
      " |      same library as this class (liblinear).\n",
      " |  \n",
      " |  SVR : Implementation of Support Vector Machine regression using libsvm:\n",
      " |      the kernel can be non-linear but its SMO algorithm does not\n",
      " |      scale to large number of samples as LinearSVC does.\n",
      " |  \n",
      " |  sklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost\n",
      " |      function as LinearSVR\n",
      " |      by adjusting the penalty and loss parameters. In addition it requires\n",
      " |      less memory, allows incremental (online) learning, and implements\n",
      " |      various loss functions and regularization regimes.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import LinearSVR\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      " |  >>> regr = make_pipeline(StandardScaler(),\n",
      " |  ...                      LinearSVR(dual=\"auto\", random_state=0, tol=1e-5))\n",
      " |  >>> regr.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('linearsvr', LinearSVR(dual='auto', random_state=0, tol=1e-05))])\n",
      " |  \n",
      " |  >>> print(regr.named_steps['linearsvr'].coef_)\n",
      " |  [18.582... 27.023... 44.357... 64.522...]\n",
      " |  >>> print(regr.named_steps['linearsvr'].intercept_)\n",
      " |  [-4...]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-2.384...]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearSVR\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual='warn', verbose=0, random_state=None, max_iter=1000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Array of weights that are assigned to individual\n",
      " |          samples. If not provided,\n",
      " |          then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.18\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          An instance of the estimator.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.svm._classes.LinearSVR, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.svm._classes.LinearSVR\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.svm._classes.LinearSVR, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.svm._classes.LinearSVR\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearSVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42e3a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify X and y\n",
    "X = train.drop(\"Lb\", axis=1)\n",
    "y = train[\"Lb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56cfddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train/ test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X,y , random_state=42,test_size=0.3, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9910dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.43677681 8.59009711 9.06080935 9.7874969 ]\n",
      "6.190861568753354\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVR().fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4ae0e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m coefs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mcoef_, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients\u001b[39m\u001b[38;5;124m\"\u001b[39m], index\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m coefs\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarh\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVR model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39maxvline(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplots_adjust(left\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n",
      "File \u001b[1;32mJ:\\anaconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\_api\\__init__.py:222\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'title'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefs = pd.DataFrame(\n",
    "    model.coef_, columns=[\"Coefficients\"], index=X.columns\n",
    ")\n",
    "\n",
    "coefs.plot(kind=\"barh\", figsize=(9, 7))\n",
    "plt.title(\"LinearSVR model\")\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)\n",
    "plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "921a00e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L avg.</th>\n",
       "      <th>Rb</th>\n",
       "      <th>Rl</th>\n",
       "      <th>A</th>\n",
       "      <th>Dd</th>\n",
       "      <th>Fs</th>\n",
       "      <th>Rt</th>\n",
       "      <th>Re</th>\n",
       "      <th>Ff</th>\n",
       "      <th>Rc</th>\n",
       "      <th>Lg</th>\n",
       "      <th>In</th>\n",
       "      <th>R</th>\n",
       "      <th>RR</th>\n",
       "      <th>Rn</th>\n",
       "      <th>HI</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.373</td>\n",
       "      <td>3.717</td>\n",
       "      <td>2.046</td>\n",
       "      <td>32.32</td>\n",
       "      <td>3.19</td>\n",
       "      <td>8.54</td>\n",
       "      <td>8.23</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.157</td>\n",
       "      <td>27.21</td>\n",
       "      <td>1106</td>\n",
       "      <td>0.115</td>\n",
       "      <td>3.524</td>\n",
       "      <td>0.438</td>\n",
       "      <td>44.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385</td>\n",
       "      <td>4.499</td>\n",
       "      <td>2.375</td>\n",
       "      <td>37.30</td>\n",
       "      <td>3.29</td>\n",
       "      <td>8.55</td>\n",
       "      <td>9.21</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.152</td>\n",
       "      <td>28.18</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.101</td>\n",
       "      <td>3.654</td>\n",
       "      <td>0.406</td>\n",
       "      <td>41.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.427</td>\n",
       "      <td>4.154</td>\n",
       "      <td>2.023</td>\n",
       "      <td>44.50</td>\n",
       "      <td>3.40</td>\n",
       "      <td>7.98</td>\n",
       "      <td>8.12</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.147</td>\n",
       "      <td>27.15</td>\n",
       "      <td>1178</td>\n",
       "      <td>0.075</td>\n",
       "      <td>4.010</td>\n",
       "      <td>0.350</td>\n",
       "      <td>34.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.488</td>\n",
       "      <td>4.333</td>\n",
       "      <td>2.347</td>\n",
       "      <td>14.47</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.57</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.156</td>\n",
       "      <td>21.01</td>\n",
       "      <td>671</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.148</td>\n",
       "      <td>0.271</td>\n",
       "      <td>29.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.408</td>\n",
       "      <td>2.871</td>\n",
       "      <td>1.762</td>\n",
       "      <td>46.72</td>\n",
       "      <td>3.34</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.150</td>\n",
       "      <td>27.31</td>\n",
       "      <td>1475</td>\n",
       "      <td>0.099</td>\n",
       "      <td>4.926</td>\n",
       "      <td>0.273</td>\n",
       "      <td>34.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.376</td>\n",
       "      <td>3.908</td>\n",
       "      <td>2.083</td>\n",
       "      <td>30.42</td>\n",
       "      <td>3.14</td>\n",
       "      <td>8.35</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.159</td>\n",
       "      <td>26.21</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.119</td>\n",
       "      <td>3.924</td>\n",
       "      <td>0.438</td>\n",
       "      <td>50.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.396</td>\n",
       "      <td>3.648</td>\n",
       "      <td>1.905</td>\n",
       "      <td>26.10</td>\n",
       "      <td>3.39</td>\n",
       "      <td>8.55</td>\n",
       "      <td>7.34</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.148</td>\n",
       "      <td>28.95</td>\n",
       "      <td>903</td>\n",
       "      <td>0.092</td>\n",
       "      <td>3.059</td>\n",
       "      <td>0.481</td>\n",
       "      <td>39.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.414</td>\n",
       "      <td>2.324</td>\n",
       "      <td>1.572</td>\n",
       "      <td>24.15</td>\n",
       "      <td>3.41</td>\n",
       "      <td>8.24</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.146</td>\n",
       "      <td>28.14</td>\n",
       "      <td>1045</td>\n",
       "      <td>0.114</td>\n",
       "      <td>3.568</td>\n",
       "      <td>0.281</td>\n",
       "      <td>32.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.381</td>\n",
       "      <td>2.304</td>\n",
       "      <td>1.789</td>\n",
       "      <td>28.77</td>\n",
       "      <td>2.92</td>\n",
       "      <td>7.65</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.171</td>\n",
       "      <td>22.30</td>\n",
       "      <td>1195</td>\n",
       "      <td>0.118</td>\n",
       "      <td>3.484</td>\n",
       "      <td>0.404</td>\n",
       "      <td>51.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.450</td>\n",
       "      <td>3.499</td>\n",
       "      <td>1.764</td>\n",
       "      <td>23.18</td>\n",
       "      <td>3.55</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.02</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.141</td>\n",
       "      <td>28.02</td>\n",
       "      <td>839</td>\n",
       "      <td>0.098</td>\n",
       "      <td>2.979</td>\n",
       "      <td>0.433</td>\n",
       "      <td>32.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.406</td>\n",
       "      <td>4.310</td>\n",
       "      <td>2.192</td>\n",
       "      <td>13.05</td>\n",
       "      <td>3.39</td>\n",
       "      <td>8.35</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.147</td>\n",
       "      <td>28.33</td>\n",
       "      <td>1209</td>\n",
       "      <td>0.158</td>\n",
       "      <td>4.102</td>\n",
       "      <td>0.392</td>\n",
       "      <td>40.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.382</td>\n",
       "      <td>2.396</td>\n",
       "      <td>1.763</td>\n",
       "      <td>30.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>8.57</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.153</td>\n",
       "      <td>28.07</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.072</td>\n",
       "      <td>3.374</td>\n",
       "      <td>0.385</td>\n",
       "      <td>43.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.387</td>\n",
       "      <td>3.485</td>\n",
       "      <td>2.033</td>\n",
       "      <td>21.29</td>\n",
       "      <td>3.16</td>\n",
       "      <td>8.17</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.158</td>\n",
       "      <td>25.84</td>\n",
       "      <td>1218</td>\n",
       "      <td>0.119</td>\n",
       "      <td>3.851</td>\n",
       "      <td>0.479</td>\n",
       "      <td>45.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    L avg.     Rb     Rl      A    Dd    Fs    Rt     Re     Ff     Rc     Lg  \\\n",
       "0    0.373  3.717  2.046  32.32  3.19  8.54  8.23  0.669  0.351  0.361  0.157   \n",
       "1    0.385  4.499  2.375  37.30  3.29  8.55  9.21  0.629  0.310  0.390  0.152   \n",
       "2    0.427  4.154  2.023  44.50  3.40  7.98  8.12  0.482  0.182  0.292  0.147   \n",
       "3    0.488  4.333  2.347  14.47  3.20  6.57  3.83  0.443  0.154  0.296  0.156   \n",
       "4    0.408  2.871  1.762  46.72  3.34  8.18  6.26  0.516  0.209  0.157  0.150   \n",
       "5    0.376  3.908  2.083  30.42  3.14  8.35  7.25  0.595  0.278  0.311  0.159   \n",
       "6    0.396  3.648  1.905  26.10  3.39  8.55  7.34  0.588  0.272  0.356  0.148   \n",
       "7    0.414  2.324  1.572  24.15  3.41  8.24  5.92  0.606  0.289  0.269  0.146   \n",
       "8    0.381  2.304  1.789  28.77  2.92  7.65  6.70  0.595  0.278  0.335  0.171   \n",
       "9    0.450  3.499  1.764  23.18  3.55  7.89  7.02  0.634  0.315  0.429  0.141   \n",
       "10   0.406  4.310  2.192  13.05  3.39  8.35  5.33  0.531  0.222  0.392  0.147   \n",
       "11   0.382  2.396  1.763  30.93  3.28  8.57  6.66  0.437  0.150  0.245  0.153   \n",
       "12   0.387  3.485  2.033  21.29  3.16  8.17  5.73  0.510  0.204  0.290  0.158   \n",
       "\n",
       "       In     R     RR     Rn     HI      S  \n",
       "0   27.21  1106  0.115  3.524  0.438  44.51  \n",
       "1   28.18  1109  0.101  3.654  0.406  41.91  \n",
       "2   27.15  1178  0.075  4.010  0.350  34.26  \n",
       "3   21.01   671  0.069  2.148  0.271  29.46  \n",
       "4   27.31  1475  0.099  4.926  0.273  34.77  \n",
       "5   26.21  1250  0.119  3.924  0.438  50.24  \n",
       "6   28.95   903  0.092  3.059  0.481  39.52  \n",
       "7   28.14  1045  0.114  3.568  0.281  32.45  \n",
       "8   22.30  1195  0.118  3.484  0.404  51.07  \n",
       "9   28.02   839  0.098  2.979  0.433  32.15  \n",
       "10  28.33  1209  0.158  4.102  0.392  40.94  \n",
       "11  28.07  1030  0.072  3.374  0.385  43.35  \n",
       "12  25.84  1218  0.119  3.851  0.479  45.62  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8af2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
